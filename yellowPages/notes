2014-08-13
==========
Am able to use scrapy to create a spider and crawl through yellow pages. Need to
determine the hierarchy of tags that are needed to extract all the relevant 
info. I think the first thing to do is to grab the business names first and 
store those. The ideal way of doing this is to create YellowPageItem objects and
store those into a JSON for future use.

Was able to extract name and street address. Need to find base XPath and just
append "suffix" paths to it and start extracting business information.

Same output:
[u'Cupcakes']
[u'4811 E Grant Rd']
[u'Beyond Bread']
[u'6260 E Speedway Blvd']
[u'Frogs Organic Bakery']
[u'7109 N Oracle Rd']
[u"Busy B's Bakery"]
[u'6781 N Thornydale Rd Ste 229']
[u'E &amp; L Southwest Cakes']
[u'3928 S 6th Ave']
[u'Nadines Bakery']
[u'4553 E Broadway Blvd']
[u'Maribelle Cakery &amp; Tatisserie']
[u'2930 N Swan Rd Ste 126']
[u'La Baguette Bakery']
[u'1797 E Prince Rd']
[u'Village Bakehouse']
[u'7882 N Oracle Rd']
[u'La Baguette Parisienne']
[u'7851 E Broadway Blvd']
[u"Norma's Cakery"]
[u'2500 S 6th Ave']
[u'Sweet Things Cupcake Shoppe']
[u'7475 N La Cholla Blvd']
[u'Sweet Tooshies Bakery']
[u'8963 E Tanque Verde Rd']
[u'Enchanted Cakes &amp; Cupcakes']
[u'10810 W Anthony Dr']
[u'Cafe Jasper']
[u'6370 N Campbell Ave Ste 160']
[u'Bagelry']
[u'2575 N Campbell Ave']
[u'Something Sweet Dessert Lounge']
[u'5319 E Speedway Blvd']
[u'Le-Delice']
[u'7245 E Tanque Verde Rd']
[u'La Palma Tortilla Factory']
[u'3624 N Oracle Rd']
[u'Cafe 54']
[u'54 E Pennington St']
[u"Dunkin' Donuts"]
[u'4676 E Grant Rd']
[u"A J's"]
[u'2805 E Skyline Dr']
[u'Albertsons']
[u'7300 N La Cholla Blvd']
[u'Sweet Things Cupcake Shoppe']
[u'4552 W Sun Quest St']
[u'Tayna Restaurant &amp; Bakery']
[u'2526 E 6th St']
[u'Perkins Restaurant &amp; Bakery']
[u'4775 E Grant Rd']
[u'Prep &amp; Pastry']
[u'3073 N Campbell Ave']
[u'Cakes By Clara']
[u'3419 N 1st Ave']
[u'Bakehouse Bread']
[u'2550 N Dragoon St']
[u'Super Duper Donuts']
[u'9572 E 29th St']

NOTE: Still need to figure out how to go to next page of results.

2014-08-20
==========
Figured out how to get next page of results. The yield of Requests with a
callback makes using rules unnecessary, but perhaps it's more scalable
and flexible to use rules (and also more challenging).

2014-08-21
==========
It may be better that the code be made flexible to adapt to grabbing links.
For now we know the format of the link and using that we create a URL for the
next request.

li_text = li.xpath('.//a/text()').extract() == 'Next' doesn't work.
This is because that the left statment yields a list of values and the right
is a string.

If yielding a Request object has a callback function, then the rules for the 
spider are not needed. 

2014-08-22
==========
Need to think of a better way to go through list of li_tags. Is this the only
way? It requires linear time search and that should be okay given that we need
to search through entire list anyways. But is it necessary to go through it
again?

Need to see what other information I need to extract for each business. Need
to crawl through specific business pages to grab website and categories as
a first step.

Want to see if my regex is sufficient to match the href links that contain
business-specific information. 

Under the python shell, imported the regex library (re) and pasted the url as
well as the regex that i'm matching the URL with and saw that the match works:

<code>
>>> import re
>>> url1 = 'http://www.yellowpages.com/tucson-az/mip/sweet-tooshies-bakery-471865645?lid=471865645'
>>> regex1 = re.compile('\d+\?lid=\d+')
>>> regex1.search(url1)
<_sre.SRE_Match object at 0x101a9d7e8>
</code>

2014-08-23
==========
Able to use rules to crawl through all business listings in a listings page.
Really cut down on code. However, not able to crawl and grab through all 100
items. Need to figure out why. There are, however, 102 requests being sent.

Can see if we can use start_requests() to fill out the form for YP

2014-08-24
==========
Looked into why the crawler only had 85 items scraped as opposed to the 100 
that existed in the results of the yellowpages. I logged the requests by having
a callback to log the specific GET requests sent to the server. It turns out 
that there are duplicate postings on the yellowpages results. Who'd have known?
Counting the duplicates, there are 15 of them and hence the discrepancy in the
two values.

Next steps are to modify the form input so that it is flexible for any kind of 
query.

Modified the fields for the item and split up the city_state field to city, 
state, and postal. That way it would be allow for easier processing. 

2014-08-25
==========
Figured out HTTP GET request that is needed for Scrapy to form a request so the
spider can be generalized to search for other categories in different locations.

Went to http://www.hurl.it/ to try out the HTTP Request and looked at the network
tab when inspecting elements for the yellowpages results. Figured out the fields
that are being populated and also realized that the HTTP request is not a POST,
but a GET. This makes sense since we're not really updating any kind of HTML page
on the server, but forming a request (with parameters being filled) and getting
the page back. The specific fields are "search_terms" and "geo_location_terms"

Also fixed a minor bug that didn't scrape an item if the city_state_string is
empty.


