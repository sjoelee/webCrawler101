2014-08-13
==========
Am able to use scrapy to create a spider and crawl through yellow pages. Need to
determine the hierarchy of tags that are needed to extract all the relevant 
info. I think the first thing to do is to grab the business names first and 
store those. The ideal way of doing this is to create YellowPageItem objects and
store those into a JSON for future use.

Was able to extract name and street address. Need to find base XPath and just
append "suffix" paths to it and start extracting business information.

Same output:
[u'Cupcakes']
[u'4811 E Grant Rd']
[u'Beyond Bread']
[u'6260 E Speedway Blvd']
[u'Frogs Organic Bakery']
[u'7109 N Oracle Rd']
[u"Busy B's Bakery"]
[u'6781 N Thornydale Rd Ste 229']
[u'E &amp; L Southwest Cakes']
[u'3928 S 6th Ave']
[u'Nadines Bakery']
[u'4553 E Broadway Blvd']
[u'Maribelle Cakery &amp; Tatisserie']
[u'2930 N Swan Rd Ste 126']
[u'La Baguette Bakery']
[u'1797 E Prince Rd']
[u'Village Bakehouse']
[u'7882 N Oracle Rd']
[u'La Baguette Parisienne']
[u'7851 E Broadway Blvd']
[u"Norma's Cakery"]
[u'2500 S 6th Ave']
[u'Sweet Things Cupcake Shoppe']
[u'7475 N La Cholla Blvd']
[u'Sweet Tooshies Bakery']
[u'8963 E Tanque Verde Rd']
[u'Enchanted Cakes &amp; Cupcakes']
[u'10810 W Anthony Dr']
[u'Cafe Jasper']
[u'6370 N Campbell Ave Ste 160']
[u'Bagelry']
[u'2575 N Campbell Ave']
[u'Something Sweet Dessert Lounge']
[u'5319 E Speedway Blvd']
[u'Le-Delice']
[u'7245 E Tanque Verde Rd']
[u'La Palma Tortilla Factory']
[u'3624 N Oracle Rd']
[u'Cafe 54']
[u'54 E Pennington St']
[u"Dunkin' Donuts"]
[u'4676 E Grant Rd']
[u"A J's"]
[u'2805 E Skyline Dr']
[u'Albertsons']
[u'7300 N La Cholla Blvd']
[u'Sweet Things Cupcake Shoppe']
[u'4552 W Sun Quest St']
[u'Tayna Restaurant &amp; Bakery']
[u'2526 E 6th St']
[u'Perkins Restaurant &amp; Bakery']
[u'4775 E Grant Rd']
[u'Prep &amp; Pastry']
[u'3073 N Campbell Ave']
[u'Cakes By Clara']
[u'3419 N 1st Ave']
[u'Bakehouse Bread']
[u'2550 N Dragoon St']
[u'Super Duper Donuts']
[u'9572 E 29th St']

NOTE: Still need to figure out how to go to next page of results.

2014-08-20
==========
Figured out how to get next page of results. The yield of Requests with a
callback makes using rules unnecessary, but perhaps it's more scalable
and flexible to use rules (and also more challenging).

2014-08-21
==========
It may be better that the code be made flexible to adapt to grabbing links.
For now we know the format of the link and using that we create a URL for the
next request.

li_text = li.xpath('.//a/text()').extract() == 'Next' doesn't work.
This is because that the left statment yields a list of values and the right
is a string.

If yielding a Request object has a callback function, then the rules for the 
spider are not needed. 

